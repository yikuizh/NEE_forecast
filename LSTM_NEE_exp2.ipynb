{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 31966,
     "status": "ok",
     "timestamp": 1655385164557,
     "user": {
      "displayName": "Yikui Zhang",
      "userId": "13264182408663850937"
     },
     "user_tz": -120
    },
    "id": "wpOtnr5Voj87"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# \n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17026,
     "status": "ok",
     "timestamp": 1655385227247,
     "user": {
      "displayName": "Yikui Zhang",
      "userId": "13264182408663850937"
     },
     "user_tz": -120
    },
    "id": "fHUfUtQLi5d7",
    "outputId": "5276d065-8810-42bc-901d-afa89b029813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: google-drive-ocamlfuse: command not found\n",
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/NEE\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Link to Google Drive root\n",
    "'''\n",
    "\n",
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd '/content/drive/My Drive/NEE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1655385230626,
     "user": {
      "displayName": "Yikui Zhang",
      "userId": "13264182408663850937"
     },
     "user_tz": -120
    },
    "id": "DpVizEgei5gS"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Import required packages\n",
    "'''\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from tensorflow import keras\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, Dense, LSTM, RepeatVector, Dropout, Input, Flatten,Concatenate\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from tensorflow import optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob,os\n",
    "import random\n",
    "import os,shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1655387930624,
     "user": {
      "displayName": "Yikui Zhang",
      "userId": "13264182408663850937"
     },
     "user_tz": -120
    },
    "id": "BFxsANIii5i_"
   },
   "outputs": [],
   "source": [
    "# data preparation without lookback -- One to One prediction (o2o)\n",
    "def data_prepare_o2o(dataset,scaler1, scaler2, shuffle):\n",
    "    \n",
    "    dataX, dataY = [], []\n",
    "    scale_feature = scaler1.transform(dataset[:,:-1])\n",
    "    scale_target = scaler2.transform(dataset[:,-1].reshape(-1, 1))\n",
    "    for i in range(len(dataset)):\n",
    "        feature = scale_feature[i]\n",
    "        dataX.append(feature)\n",
    "        target = scale_target[i]\n",
    "        dataY.append(target)\n",
    "        \n",
    "    dataX, dataY = np.array(dataX), np.array(dataY)\n",
    "    number_list = np.arange(0,len(dataX),1)\n",
    "    random.shuffle(number_list) \n",
    "    dataX_new = []\n",
    "    for index in number_list:\n",
    "        dataX_new.append(dataX[index])\n",
    "    dataY_new = []\n",
    "    for index in number_list:\n",
    "        dataY_new.append(dataY[index])\n",
    "\n",
    "    if shuffle:\n",
    "        return np.array(dataX_new), np.array(dataY_new)\n",
    "    else:\n",
    "        return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1655391454988,
     "user": {
      "displayName": "Yikui Zhang",
      "userId": "13264182408663850937"
     },
     "user_tz": -120
    },
    "id": "J9jrOSmejL-1"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Data load\n",
    "'''\n",
    "\n",
    "dataset = pd.read_csv(r'/content/drive/My Drive/NEE/data2_ML_new.csv',index_col = 0)\n",
    "\n",
    "dynamic_feature_column = dataset.columns[8:18]\n",
    "static_feature_column1 = dataset.columns[2:8]\n",
    "static_feature_column2 = dataset.columns[19:]\n",
    "target = dataset['NEE']\n",
    "\n",
    "all_sites = dataset['Site'].unique()\n",
    "\n",
    "result2 = pd.DataFrame(columns = all_sites, index = ['rmse_test', 'corr_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1655391456312,
     "user": {
      "displayName": "Yikui Zhang",
      "userId": "13264182408663850937"
     },
     "user_tz": -120
    },
    "id": "dUsfxNvCkTEa"
   },
   "outputs": [],
   "source": [
    "# train and test split on all sites\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle = True,random_state = 1) # n_splites can be changed based on different folds required; \n",
    "train_index_list = []\n",
    "test_index_list = []\n",
    "# train_index as gauged location for training, test_index as ungauged catchments for testing\n",
    "for train_index, test_index in kf.split(all_sites):\n",
    "    train_index_list.append(train_index)\n",
    "    test_index_list.append(test_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 720026,
     "status": "ok",
     "timestamp": 1655392188194,
     "user": {
      "displayName": "Yikui Zhang",
      "userId": "13264182408663850937"
     },
     "user_tz": -120
    },
    "id": "V3PsuwTKjMBc",
    "outputId": "61999bae-6028-48b8-d6de-48f3ca621820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Train Score: 0.93 RMSE\n",
      "Test Score: 1.83 RMSE\n",
      "Train corr: 0.91 corr\n",
      "Test corr: 0.59 corr\n",
      "2\n",
      "Train Score: 1.13 RMSE\n",
      "Test Score: 2.12 RMSE\n",
      "Train corr: 0.86 corr\n",
      "Test corr: 0.52 corr\n",
      "3\n",
      "Train Score: 0.94 RMSE\n",
      "Test Score: 1.93 RMSE\n",
      "Train corr: 0.91 corr\n",
      "Test corr: 0.60 corr\n",
      "4\n",
      "Train Score: 1.01 RMSE\n",
      "Test Score: 2.19 RMSE\n",
      "Train corr: 0.89 corr\n",
      "Test corr: 0.53 corr\n",
      "5\n",
      "Train Score: 1.01 RMSE\n",
      "Test Score: 1.96 RMSE\n",
      "Train corr: 0.90 corr\n",
      "Test corr: 0.51 corr\n"
     ]
    }
   ],
   "source": [
    "### EXP2 ###\n",
    "dynamic_dataset = dataset.drop(columns=['Unnamed: 0.1',  'VegType', 'lat', 'lon', 'timeStamp', 'Year',\n",
    "           'Month', 'classKoppen', 'SWC', 'soilType', 'high_vegType', 'low_vegType',\n",
    "           'high_vegCover', 'low_vegCover', 'Elavation'])\n",
    "\n",
    "# LSTM cell: 32,64,96\n",
    "# batch size:5, 10, 16, 32, 64, 128 -- try batch size 128 first -- over 15000 samples for each train\n",
    "\n",
    "for i in range(len(train_index_list)):\n",
    "    num_ite = i + 1 # count the number of folder\n",
    "    print (num_ite)\n",
    "    #site_dataset = dataset.loc[dataset['Site'] == train_index_list[i]]\n",
    "\n",
    "    #dynamic_feature = site_dataset[dynamic_feature_column]\n",
    "    #static_feature1 = site_dataset[static_feature_column1]\n",
    "    #static_feature2 = site_dataset[static_feature_column2]\n",
    "\n",
    "    # split into train and test sets\n",
    "    #train_size = int(len(dynamic_dataset.values) * 0.75)\n",
    "    #test_size = len(dynamic_dataset.values) - train_size\n",
    "    #train, test = dynamic_dataset.values[0:train_size,:], dynamic_dataset.values[train_size:len(dynamic_dataset.values),:]\n",
    "\n",
    "    train_sites = dynamic_dataset.Site.unique()[train_index_list[i]]\n",
    "    train_dynamic_dataset = dynamic_dataset[dataset.Site.isin(train_sites)].drop(columns=['Site']).values\n",
    "\n",
    "    test_sites = dynamic_dataset.Site.unique()[test_index_list[i]]\n",
    "    test_dynamic_dataset = dynamic_dataset[dataset.Site.isin(test_sites)].drop(columns=['Site']).values\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_x.fit(train_dynamic_dataset[:,:-1])\n",
    "\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_y.fit(train_dynamic_dataset[:,-1].reshape(-1, 1))\n",
    "\n",
    "    # train and test dataset\n",
    "    trainX, trainY = data_prepare_o2o(train_dynamic_dataset, scaler_x, scaler_y, True)\n",
    "    testX, testY = data_prepare_o2o(test_dynamic_dataset, scaler_x, scaler_y, True)\n",
    "    \n",
    "    '''\n",
    "    Valina LSTM\n",
    "    '''\n",
    "    # define customer optimizer\n",
    "    #RMSprop= optimizers.RMSprop(lr=0.0001)\n",
    "    # define special callbacks\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=15, cooldown=30, min_lr=1e-8)\n",
    "    # define early stop\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=0)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape=(trainX.shape[1], 1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(trainX, trainY, epochs=300, validation_split=0.1, batch_size=128, callbacks=[reduce_lr, early_stopping], verbose=0)\n",
    "    \n",
    "    # save the model\n",
    "    model.save(r'/content/drive/My Drive/NEE/nee_regional_%s.h5' % (num_ite)) \n",
    "\n",
    "    ### evaluate the overall performance ###\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    \n",
    "    trainPredict = scaler_y.inverse_transform(trainPredict)\n",
    "    trainY = scaler_y.inverse_transform(trainY)\n",
    "    testPredict = scaler_y.inverse_transform(testPredict)\n",
    "    testY = scaler_y.inverse_transform(testY)\n",
    "\n",
    "    # calculate root mean squared error\n",
    "    trainScore = math.sqrt(mean_squared_error(trainY.squeeze(), trainPredict[:,0]))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = math.sqrt(mean_squared_error(testY.squeeze(), testPredict[:,0]))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "    # calculate correlation score\n",
    "    corr_train, _ = pearsonr(trainY.squeeze(), trainPredict[:,0])\n",
    "    print('Train corr: %.2f corr' % (corr_train))\n",
    "    corr_test, _ = pearsonr(testY.squeeze(), testPredict[:,0])\n",
    "    print('Test corr: %.2f corr' % (corr_test))\n",
    "\n",
    "    ### for individual site ###\n",
    "    for k in range(len(test_sites)):\n",
    "        # make predictions\n",
    "        test_single_dynamic_dataset = dynamic_dataset.loc[dynamic_dataset['Site'] == test_sites[k]].drop(columns=['Site']).values\n",
    "        \n",
    "        testX, testY = data_prepare_o2o(test_single_dynamic_dataset, scaler_x, scaler_y, True)\n",
    "        testPredict = model.predict(testX)\n",
    "        #trainPredict = scaler_y.inverse_transform(trainPredict)\n",
    "        #trainY = scaler_y.inverse_transform(trainY)\n",
    "        testPredict = scaler_y.inverse_transform(testPredict)\n",
    "        testY = scaler_y.inverse_transform(testY)\n",
    "\n",
    "        # calculate root mean squared error\n",
    "\n",
    "        testScore = math.sqrt(mean_squared_error(testY.squeeze(), testPredict[:,0]))\n",
    "        #print('Test Score: %.2f RMSE' % (testScore))\n",
    "        # calculate correlation score\n",
    "        corr_test, _ = pearsonr(testY.squeeze(), testPredict[:,0])\n",
    "        result2[test_sites[k]] = [testScore,corr_test]\n",
    "    \n",
    "    #print (all_sites[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1655392188194,
     "user": {
      "displayName": "Yikui Zhang",
      "userId": "13264182408663850937"
     },
     "user_tz": -120
    },
    "id": "wsBQcFrgjMEE"
   },
   "outputs": [],
   "source": [
    "result2.to_csv(r'/content/drive/My Drive/NEE/result_exp2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzFXHmHMA3xG"
   },
   "outputs": [],
   "source": [
    "# SAVE model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP6AZN3kDm9VyiASRnmUoFh",
   "collapsed_sections": [],
   "name": "RegionalNEE.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
