{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca068e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import required packages\n",
    "'''\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, Dense, LSTM, RepeatVector, Dropout, Input, Flatten,Concatenate\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from tensorflow import optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob,os\n",
    "import random\n",
    "import os,shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de7712da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation without lookback -- One to One prediction (o2o)\n",
    "def data_prepare_o2o(dataset,scaler1, scaler2, shuffle):\n",
    "    \n",
    "    dataX, dataY = [], []\n",
    "    scale_feature = scaler1.transform(dataset[:,:-1])\n",
    "    scale_target = scaler2.transform(dataset[:,-1].reshape(-1, 1))\n",
    "    for i in range(len(dataset)):\n",
    "        feature = scale_feature[i]\n",
    "        dataX.append(feature)\n",
    "        target = scale_target[i]\n",
    "        dataY.append(target)\n",
    "        \n",
    "    dataX, dataY = np.array(dataX), np.array(dataY)\n",
    "    number_list = np.arange(0,len(dataX),1)\n",
    "    random.shuffle(number_list) \n",
    "    dataX_new = []\n",
    "    for index in number_list:\n",
    "        dataX_new.append(dataX[index])\n",
    "    dataY_new = []\n",
    "    for index in number_list:\n",
    "        dataY_new.append(dataY[index])\n",
    "\n",
    "    if shuffle:\n",
    "        return np.array(dataX_new), np.array(dataY_new)\n",
    "    else:\n",
    "        return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3798ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data load\n",
    "'''\n",
    "\n",
    "dataset = pd.read_csv(r'C:\\Users\\Macbook Air\\Desktop\\NEE_forecast\\data\\data2_ML_new.csv',index_col = 0)\n",
    "\n",
    "dynamic_feature_column = dataset.columns[8:18]\n",
    "static_feature_column1 = dataset.columns[2:8]\n",
    "static_feature_column2 = dataset.columns[19:]\n",
    "target = dataset['NEE']\n",
    "\n",
    "all_sites = dataset['Site'].unique()\n",
    "\n",
    "result = pd.DataFrame(columns = all_sites, index = ['rmse_train', 'rmse_test', 'corr_train','corr_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae3f4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_dataset = dataset.loc[dataset['Site'] == all_sites[0]]\n",
    "\n",
    "#dynamic_feature = site_dataset[dynamic_feature_column]\n",
    "#static_feature1 = site_dataset[static_feature_column1]\n",
    "#static_feature2 = site_dataset[static_feature_column2]\n",
    "\n",
    "dynamic_dataset = site_dataset.drop(columns=['Unnamed: 0.1', 'Site', 'VegType', 'lat', 'lon', 'timeStamp', 'Year',\n",
    "       'Month', 'classKoppen', 'SWC', 'soilType', 'high_vegType', 'low_vegType',\n",
    "       'high_vegCover', 'low_vegCover', 'Elavation'])\n",
    "\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dynamic_dataset.values) * 0.75)\n",
    "test_size = len(dynamic_dataset.values) - train_size\n",
    "train, test = dynamic_dataset.values[0:train_size,:], dynamic_dataset.values[train_size:len(dynamic_dataset.values),:]\n",
    "\n",
    "# first split train/test and then apply scaler\n",
    "scaler_x = StandardScaler()\n",
    "scaler_x.fit(train[:,:-1])\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "scaler_y.fit(train[:,-1].reshape(-1, 1))\n",
    "\n",
    "# train and test dataset\n",
    "trainX, trainY = data_prepare_o2o(train, scaler_x, scaler_y, True)\n",
    "testX, testY = data_prepare_o2o(test, scaler_x, scaler_y, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c13e1583",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08ef9b70",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR-SLu\n",
      "AT-Neu\n",
      "AU-ASM\n",
      "AU-Ade\n",
      "AU-Cpr\n",
      "AU-Cum\n",
      "AU-DaP\n",
      "AU-DaS\n",
      "AU-Dry\n",
      "AU-Emr\n",
      "AU-Fog\n",
      "AU-GWW\n",
      "WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000015C2E7FE790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "AU-Gin\n",
      "AU-How\n",
      "AU-Lox\n",
      "AU-RDF\n",
      "AU-Rig\n",
      "WARNING:tensorflow:5 out of the last 20 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000015C2FE39040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "AU-Rob\n",
      "AU-Stp\n",
      "AU-TTE\n",
      "AU-Tum\n",
      "AU-Wac\n",
      "AU-Whr\n",
      "AU-Wom\n",
      "AU-Ync\n",
      "BE-Bra\n",
      "BE-Lon\n",
      "BE-Vie\n",
      "BR-Sa3\n",
      "CA-Gro\n",
      "CA-Obs\n",
      "CA-Qfo\n",
      "CA-SF1\n",
      "CA-SF2\n",
      "CA-SF3\n",
      "CA-TP1\n",
      "CA-TP2\n",
      "CA-TP3\n",
      "CA-TP4\n",
      "CA-TPD\n",
      "CH-Cha\n",
      "CH-Dav\n",
      "CH-Fru\n",
      "CN-Cng\n",
      "CN-Du2\n",
      "CN-Du3\n",
      "CN-HaM\n",
      "CZ-wet\n",
      "DE-Geb\n",
      "DE-Gri\n",
      "DE-Hai\n",
      "DE-Kli\n",
      "DE-Lkb\n",
      "DE-Lnf\n",
      "DE-Obe\n",
      "DE-RuR\n",
      "DE-RuS\n",
      "DE-Seh\n",
      "DE-SfN\n",
      "DE-Tha\n",
      "DE-Zrk\n",
      "DK-Eng\n",
      "DK-Fou\n",
      "DK-Sor\n",
      "ES-Amo\n",
      "ES-LJu\n",
      "ES-LgS\n",
      "FI-Hyy\n",
      "FI-Jok\n",
      "FI-Let\n",
      "FI-Lom\n",
      "FI-Sod\n",
      "FR-Gri\n",
      "FR-LBr\n",
      "FR-Pue\n",
      "GH-Ank\n",
      "GL-ZaF\n",
      "IT-BCi\n",
      "IT-CA1\n",
      "IT-CA2\n",
      "IT-CA3\n",
      "IT-Col\n",
      "IT-Cp2\n",
      "IT-Cpz\n",
      "IT-Isp\n",
      "IT-Lav\n",
      "IT-MBo\n",
      "IT-Noe\n",
      "IT-PT1\n",
      "IT-Ren\n",
      "IT-Ro2\n",
      "IT-SR2\n",
      "IT-SRo\n",
      "IT-Tor\n",
      "MY-PSO\n",
      "NL-Hor\n",
      "NL-Loo\n",
      "RU-Cok\n",
      "RU-Fyo\n",
      "RU-Ha1\n",
      "RU-SkP\n",
      "RU-Tks\n",
      "RU-Vrk\n",
      "SD-Dem\n",
      "SJ-Adv\n",
      "SN-Dhr\n",
      "US-AR1\n",
      "US-AR2\n",
      "US-ARM\n",
      "US-ARb\n",
      "US-ARc\n",
      "US-Atq\n",
      "US-Blo\n",
      "US-CRT\n",
      "US-Cop\n",
      "US-GBT\n",
      "US-GLE\n",
      "US-Goo\n",
      "US-IB2\n",
      "US-Ivo\n",
      "US-KS1\n",
      "US-KS2\n",
      "US-LWW\n",
      "US-Lin\n",
      "US-Los\n",
      "US-MMS\n",
      "US-Me1\n",
      "US-Me2\n",
      "US-Me4\n",
      "US-Me5\n",
      "US-NR1\n",
      "US-Ne1\n",
      "US-Ne2\n",
      "US-Ne3\n",
      "US-Oho\n",
      "US-Prr\n"
     ]
    }
   ],
   "source": [
    "### EXP1 ###\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "for i in range(len(all_sites)):\n",
    "    site_dataset = dataset.loc[dataset['Site'] == all_sites[i]]\n",
    "\n",
    "    #dynamic_feature = site_dataset[dynamic_feature_column]\n",
    "    #static_feature1 = site_dataset[static_feature_column1]\n",
    "    #static_feature2 = site_dataset[static_feature_column2]\n",
    "\n",
    "    dynamic_dataset = site_dataset.drop(columns=['Unnamed: 0.1', 'Site', 'VegType', 'lat', 'lon', 'timeStamp', 'Year',\n",
    "           'Month', 'classKoppen', 'SWC', 'soilType', 'high_vegType', 'low_vegType',\n",
    "           'high_vegCover', 'low_vegCover', 'Elavation'])\n",
    "\n",
    "\n",
    "    # split into train and test sets\n",
    "    train_size = int(len(dynamic_dataset.values) * 0.75)\n",
    "    test_size = len(dynamic_dataset.values) - train_size\n",
    "    train, test = dynamic_dataset.values[0:train_size,:], dynamic_dataset.values[train_size:len(dynamic_dataset.values),:]\n",
    "    \n",
    "    # first split train/test and then apply scaler\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_x.fit(train[:,:-1])\n",
    "\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_y.fit(train[:,-1].reshape(-1, 1))\n",
    "    \n",
    "    # train and test dataset\n",
    "    trainX, trainY = data_prepare_o2o(train, scaler_x, scaler_y, True)\n",
    "    testX, testY = data_prepare_o2o(test, scaler_x, scaler_y, True)\n",
    "    \n",
    "    '''\n",
    "    Valina LSTM\n",
    "    '''\n",
    "    # define customer optimizer\n",
    "    #RMSprop= optimizers.RMSprop(lr=0.0001)\n",
    "    # define special callbacks\n",
    "    #reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=15, cooldown=30, min_lr=1e-8)\n",
    "    # define early stop\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=0)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape=(trainX.shape[1], 1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(trainX, trainY, epochs=300, validation_split=0.1, batch_size=5, callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    \n",
    "    trainPredict = scaler_y.inverse_transform(trainPredict)\n",
    "    trainY = scaler_y.inverse_transform(trainY)\n",
    "    testPredict = scaler_y.inverse_transform(testPredict)\n",
    "    testY = scaler_y.inverse_transform(testY)\n",
    "\n",
    "    # calculate root mean squared error\n",
    "    trainScore = math.sqrt(mean_squared_error(trainY.squeeze(), trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = math.sqrt(mean_squared_error(testY.squeeze(), testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "    # calculate correlation score\n",
    "    corr_train, _ = pearsonr(trainY.squeeze(), trainPredict[:,0])\n",
    "    corr_test, _ = pearsonr(testY.squeeze(), testPredict[:,0])\n",
    "    \n",
    "    result[all_sites[i]] = [trainScore, testScore, corr_train, corr_test]\n",
    "    \n",
    "    print (all_sites[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c31a2d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(r'C:\\Users\\Macbook Air\\Desktop\\NEE_forecast\\result\\result_exp1_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e6fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
